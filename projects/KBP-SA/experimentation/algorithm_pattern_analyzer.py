"""
Algorithm Pattern Analyzer - KBP-SA
Sistema para aprender y predecir qu√© patrones de algoritmos generan ejecuciones m√°s r√°pidas
"""

import json
import re
from typing import Dict, List, Tuple, Optional
from dataclasses import dataclass
from pathlib import Path
import numpy as np


@dataclass
class AlgorithmFeatures:
    """Caracter√≠sticas extra√≠das de un algoritmo"""
    constructor: str  # GreedyByRatio, GreedyByWeight, GreedyByValue, RandomConstruct
    operators: List[str]  # FlipWorstItem, FlipBestItem, TwoExchange, OneExchange
    acceptance_criteria: Optional[str]  # Metropolis, Improving, None
    has_loop: bool  # Si tiene MIENTRAS o APLICAR_HASTA_NO_MEJORAR
    loop_budget: Optional[int]  # N√∫mero de iteraciones si tiene MIENTRAS
    stagnation_limit: Optional[int]  # L√≠mite de estancamiento si tiene APLICAR_HASTA_NO_MEJORAR
    complexity_score: float  # Score estimado de complejidad


@dataclass
class AlgorithmPerformance:
    """Datos de rendimiento de un algoritmo"""
    algorithm_name: str
    features: AlgorithmFeatures
    avg_time_per_experiment: float
    max_time_per_experiment: float
    min_time_per_experiment: float
    timeout_count: int
    total_experiments: int


class AlgorithmPatternAnalyzer:
    """
    Analizador de patrones para identificar caracter√≠sticas que llevan a ejecuciones r√°pidas
    """

    def __init__(self):
        """Inicializa el analizador con conocimiento base"""

        # Conocimiento base de complejidad relativa (basado en evidencia emp√≠rica)
        self.operator_complexity = {
            'FlipWorstItem': 1.0,  # O(n) - simple flip
            'FlipBestItem': 1.0,   # O(n) - simple flip
            'OneExchange': 2.0,    # O(n) - swap simple
            'TwoExchange': 3.0,    # O(n¬≤) - m√°s complejo pero efectivo
        }

        self.constructor_speed = {
            'RandomConstruct': 1.0,      # O(n) - m√°s r√°pido
            'GreedyByWeight': 2.0,       # O(n log n) - sorting
            'GreedyByValue': 2.0,        # O(n log n) - sorting
            'GreedyByRatio': 2.5,        # O(n log n) - sorting + ratio calculation
        }

        self.acceptance_overhead = {
            'Improving': 1.0,      # Solo acepta mejoras - r√°pido
            'Metropolis': 3.0,     # Acepta peores - m√°s evaluaciones
            None: 1.0              # Sin criterio de aceptaci√≥n
        }

        # Base de datos de patrones observados
        self.pattern_database: List[AlgorithmPerformance] = []

    def extract_features(self, pseudocode: str) -> AlgorithmFeatures:
        """
        Extrae caracter√≠sticas de un algoritmo a partir de su pseudoc√≥digo

        Args:
            pseudocode: Pseudoc√≥digo completo del algoritmo

        Returns:
            AlgorithmFeatures con todas las caracter√≠sticas extra√≠das
        """
        # Extraer constructor
        constructor_match = re.search(r'CONSTRUIR_VORAZ usando (\w+)', pseudocode)
        constructor = constructor_match.group(1) if constructor_match else 'Unknown'

        # Extraer operadores
        operators = []
        operator_patterns = [
            r'LLAMAR (\w+)',
            r'BUSQUEDA_LOCAL en (\w+)',
        ]
        for pattern in operator_patterns:
            matches = re.findall(pattern, pseudocode)
            operators.extend(matches)

        # Eliminar duplicados manteniendo orden
        operators = list(dict.fromkeys(operators))

        # Extraer criterio de aceptaci√≥n
        acceptance = None
        acceptance_match = re.search(r'aceptaci√≥n: (\w+)', pseudocode)
        if acceptance_match:
            acceptance = acceptance_match.group(1)

        # Detectar loops
        has_loop = 'MIENTRAS' in pseudocode or 'APLICAR_HASTA_NO_MEJORAR' in pseudocode

        # Extraer presupuesto de iteraciones
        loop_budget = None
        budget_match = re.search(r'presupuesto: (\d+) iteraciones', pseudocode)
        if budget_match:
            loop_budget = int(budget_match.group(1))

        # Extraer l√≠mite de estancamiento
        stagnation_limit = None
        stagnation_match = re.search(r'Stagnation=(\d+)', pseudocode)
        if stagnation_match:
            stagnation_limit = int(stagnation_match.group(1))

        # Calcular score de complejidad
        complexity_score = self._calculate_complexity_score(
            constructor, operators, acceptance, loop_budget, stagnation_limit
        )

        return AlgorithmFeatures(
            constructor=constructor,
            operators=operators,
            acceptance_criteria=acceptance,
            has_loop=has_loop,
            loop_budget=loop_budget,
            stagnation_limit=stagnation_limit,
            complexity_score=complexity_score
        )

    def _calculate_complexity_score(
        self,
        constructor: str,
        operators: List[str],
        acceptance: Optional[str],
        loop_budget: Optional[int],
        stagnation_limit: Optional[int]
    ) -> float:
        """
        Calcula un score de complejidad estimada del algoritmo

        Score m√°s alto = m√°s lento esperado
        Score m√°s bajo = m√°s r√°pido esperado
        """
        score = 0.0

        # Constructor base
        score += self.constructor_speed.get(constructor, 2.0)

        # Operadores (suma de complejidades)
        for op in operators:
            score += self.operator_complexity.get(op, 2.0)

        # Criterio de aceptaci√≥n
        score *= self.acceptance_overhead.get(acceptance, 1.0)

        # Presupuesto de iteraciones
        if loop_budget:
            score *= (1 + loop_budget / 1000)  # M√°s iteraciones = m√°s lento

        if stagnation_limit:
            score *= (1 + stagnation_limit / 100)  # M√°s estancamiento = m√°s lento

        return score

    def add_observed_performance(
        self,
        algorithm_name: str,
        pseudocode: str,
        experiment_times: List[float],
        timeout_count: int = 0
    ):
        """
        Agrega datos de rendimiento observado a la base de datos

        Args:
            algorithm_name: Nombre del algoritmo
            pseudocode: Pseudoc√≥digo completo
            experiment_times: Lista de tiempos de experimentos (en segundos)
            timeout_count: N√∫mero de experimentos que hicieron timeout
        """
        features = self.extract_features(pseudocode)

        # Filtrar timeouts (asumiendo que son >60s)
        valid_times = [t for t in experiment_times if t < 60]

        if not valid_times:
            # Todos fueron timeouts, usar tiempo estimado alto
            avg_time = 60.0
            max_time = 60.0
            min_time = 60.0
        else:
            avg_time = np.mean(valid_times)
            max_time = np.max(valid_times)
            min_time = np.min(valid_times)

        performance = AlgorithmPerformance(
            algorithm_name=algorithm_name,
            features=features,
            avg_time_per_experiment=avg_time,
            max_time_per_experiment=max_time,
            min_time_per_experiment=min_time,
            timeout_count=timeout_count,
            total_experiments=len(experiment_times)
        )

        self.pattern_database.append(performance)

    def predict_speed_category(self, pseudocode: str) -> Tuple[str, float, Dict]:
        """
        Predice la categor√≠a de velocidad de un algoritmo

        Args:
            pseudocode: Pseudoc√≥digo del algoritmo

        Returns:
            Tuple de (categor√≠a, score, detalles)
            - categor√≠a: 'R√ÅPIDO', 'MEDIO', 'LENTO'
            - score: Score de complejidad (menor = m√°s r√°pido)
            - detalles: Diccionario con detalles del an√°lisis
        """
        features = self.extract_features(pseudocode)
        score = features.complexity_score

        # Categorizar basado en score
        if score < 10:
            category = 'R√ÅPIDO'
        elif score < 30:
            category = 'MEDIO'
        else:
            category = 'LENTO'

        # Generar detalles
        details = {
            'constructor': features.constructor,
            'operators': features.operators,
            'acceptance': features.acceptance_criteria,
            'loop_budget': features.loop_budget,
            'stagnation_limit': features.stagnation_limit,
            'complexity_score': score,
            'estimated_time_per_experiment': self._estimate_time(score)
        }

        return category, score, details

    def _estimate_time(self, complexity_score: float) -> str:
        """Estima tiempo basado en score de complejidad"""
        if complexity_score < 10:
            return '0.1-2s'
        elif complexity_score < 20:
            return '2-10s'
        elif complexity_score < 30:
            return '10-30s'
        else:
            return '>30s (posible timeout)'

    def rank_algorithms(self, algorithms: List[Tuple[str, str]]) -> List[Tuple[str, str, float, str]]:
        """
        Rankea algoritmos por velocidad esperada

        Args:
            algorithms: Lista de tuplas (nombre, pseudocode)

        Returns:
            Lista ordenada de tuplas (nombre, pseudocode, score, categor√≠a)
            Ordenado de m√°s r√°pido a m√°s lento
        """
        ranked = []

        for name, pseudocode in algorithms:
            category, score, _ = self.predict_speed_category(pseudocode)
            ranked.append((name, pseudocode, score, category))

        # Ordenar por score (menor = m√°s r√°pido)
        ranked.sort(key=lambda x: x[2])

        return ranked

    def filter_fast_algorithms(
        self,
        algorithms: List[Tuple[str, str]],
        max_complexity_score: float = 15.0
    ) -> List[Tuple[str, str]]:
        """
        Filtra algoritmos para quedarse solo con los r√°pidos

        Args:
            algorithms: Lista de tuplas (nombre, pseudocode)
            max_complexity_score: Score m√°ximo de complejidad permitido

        Returns:
            Lista filtrada de algoritmos r√°pidos
        """
        fast_algorithms = []

        for name, pseudocode in algorithms:
            _, score, _ = self.predict_speed_category(pseudocode)
            if score <= max_complexity_score:
                fast_algorithms.append((name, pseudocode))

        return fast_algorithms

    def analyze_pattern_correlations(self) -> Dict:
        """
        Analiza correlaciones entre caracter√≠sticas y tiempos de ejecuci√≥n

        Returns:
            Diccionario con estad√≠sticas de correlaciones
        """
        if not self.pattern_database:
            return {'error': 'No hay datos en la base de patrones'}

        # Agrupar por caracter√≠sticas
        by_constructor = {}
        by_operator = {}
        by_acceptance = {}

        for perf in self.pattern_database:
            # Por constructor
            if perf.features.constructor not in by_constructor:
                by_constructor[perf.features.constructor] = []
            by_constructor[perf.features.constructor].append(perf.avg_time_per_experiment)

            # Por operador
            for op in perf.features.operators:
                if op not in by_operator:
                    by_operator[op] = []
                by_operator[op].append(perf.avg_time_per_experiment)

            # Por criterio de aceptaci√≥n
            acc = perf.features.acceptance_criteria or 'None'
            if acc not in by_acceptance:
                by_acceptance[acc] = []
            by_acceptance[acc].append(perf.avg_time_per_experiment)

        # Calcular estad√≠sticas
        stats = {
            'constructors': {k: {'avg': np.mean(v), 'std': np.std(v), 'count': len(v)}
                           for k, v in by_constructor.items()},
            'operators': {k: {'avg': np.mean(v), 'std': np.std(v), 'count': len(v)}
                        for k, v in by_operator.items()},
            'acceptance': {k: {'avg': np.mean(v), 'std': np.std(v), 'count': len(v)}
                         for k, v in by_acceptance.items()},
        }

        return stats

    def generate_report(self, output_file: str):
        """
        Genera un reporte completo de an√°lisis de patrones

        Args:
            output_file: Ruta del archivo markdown de salida
        """
        correlations = self.analyze_pattern_correlations()

        with open(output_file, 'w', encoding='utf-8') as f:
            f.write("# An√°lisis de Patrones de Algoritmos - KBP-SA\n\n")
            f.write(f"**Algoritmos analizados**: {len(self.pattern_database)}\n\n")
            f.write("---\n\n")

            # Verificar si hay datos
            if 'error' in correlations:
                f.write("## ‚ö†Ô∏è Sin Datos\n\n")
                f.write(f"{correlations['error']}\n\n")
                return

            # Constructores
            if 'constructors' in correlations and correlations['constructors']:
                f.write("## üìä Rendimiento por Constructor\n\n")
                f.write("| Constructor | Tiempo Promedio | Desv. Est√°ndar | Muestras |\n")
                f.write("|-------------|-----------------|----------------|----------|\n")
                for constructor, stats in sorted(correlations['constructors'].items(),
                                                key=lambda x: x[1]['avg']):
                    f.write(f"| {constructor} | {stats['avg']:.3f}s | {stats['std']:.3f}s | {stats['count']} |\n")
                f.write("\n")

            # Operadores
            if 'operators' in correlations and correlations['operators']:
                f.write("## üîß Rendimiento por Operador\n\n")
                f.write("| Operador | Tiempo Promedio | Desv. Est√°ndar | Muestras |\n")
                f.write("|----------|-----------------|----------------|----------|\n")
                for operator, stats in sorted(correlations['operators'].items(),
                                             key=lambda x: x[1]['avg']):
                    f.write(f"| {operator} | {stats['avg']:.3f}s | {stats['std']:.3f}s | {stats['count']} |\n")
                f.write("\n")

            # Criterios de aceptaci√≥n
            if 'acceptance' in correlations and correlations['acceptance']:
                f.write("## ‚úÖ Rendimiento por Criterio de Aceptaci√≥n\n\n")
                f.write("| Criterio | Tiempo Promedio | Desv. Est√°ndar | Muestras |\n")
                f.write("|----------|-----------------|----------------|----------|\n")
                for acceptance, stats in sorted(correlations['acceptance'].items(),
                                               key=lambda x: x[1]['avg']):
                    f.write(f"| {acceptance} | {stats['avg']:.3f}s | {stats['std']:.3f}s | {stats['count']} |\n")
                f.write("\n")

            # Recomendaciones
            f.write("## üéØ Recomendaciones para Algoritmos R√°pidos\n\n")

            # Mejor constructor
            if 'constructors' in correlations and correlations['constructors']:
                best_constructor = min(correlations['constructors'].items(),
                                      key=lambda x: x[1]['avg'])
                f.write(f"- **Mejor Constructor**: {best_constructor[0]} ({best_constructor[1]['avg']:.3f}s)\n")

            # Mejor operador
            if 'operators' in correlations and correlations['operators']:
                best_operator = min(correlations['operators'].items(),
                                   key=lambda x: x[1]['avg'])
                f.write(f"- **Mejor Operador**: {best_operator[0]} ({best_operator[1]['avg']:.3f}s)\n")

            # Mejor criterio
            if 'acceptance' in correlations and correlations['acceptance']:
                best_acceptance = min(correlations['acceptance'].items(),
                                     key=lambda x: x[1]['avg'])
                f.write(f"- **Mejor Criterio**: {best_acceptance[0]} ({best_acceptance[1]['avg']:.3f}s)\n")

            f.write("\n---\n\n")
            f.write("**Generado por**: AlgorithmPatternAnalyzer\n")
