# Testing - GCP con Iterated Local Search

**Directorio**: `tests/`  
**Suite de tests**: 42+ tests unitarios y de integraciÃ³n  
**Cobertura objetivo**: >90%

---

## ðŸ“‹ DescripciÃ³n General

Esta suite de tests valida la correctitud de todos los componentes del proyecto GCP-ILS:

- **Core**: Problema, SoluciÃ³n, Evaluador
- **Operadores**: Constructivos, Mejora, PerturbaciÃ³n
- **MetaheurÃ­stica**: Iterated Local Search
- **IntegraciÃ³n**: Flujos completos end-to-end

---

## ðŸ“‚ Estructura de Archivos

```
tests/
â”œâ”€â”€ __init__.py              # Inicializador del paquete
â”œâ”€â”€ conftest.py              # ConfiguraciÃ³n y fixtures compartidas (opcional)
â”œâ”€â”€ test_core.py             # Tests de Core (15+ tests)
â”œâ”€â”€ test_operators.py        # Tests de Operadores (20+ tests)
â”œâ”€â”€ test_ils.py              # Tests de ILS (10+ tests)
â””â”€â”€ test_integration.py      # Tests de integraciÃ³n (opcional)

scripts/
â”œâ”€â”€ test_quick.py            # ValidaciÃ³n rÃ¡pida (~10s)
â””â”€â”€ run_tests.py             # Script para ejecutar tests con opciones

PROJECT_ROOT/
â”œâ”€â”€ run_tests.py             # Alias para script de tests
â””â”€â”€ TESTING_SUMMARY.md       # DocumentaciÃ³n completa
```

---

## ðŸš€ Inicio RÃ¡pido

### 1. InstalaciÃ³n de Dependencias

```bash
pip install pytest pytest-cov numpy
```

### 2. ValidaciÃ³n RÃ¡pida (10 segundos)

```bash
python scripts/test_quick.py
```

**Salida esperada**:
```
============================================================
  VALIDACIÃ“N RÃPIDA - GCP con ILS
  Verifica funcionamiento bÃ¡sico de componentes
============================================================

[1/5] Imports...
âœ“ Imports de core exitosos

[2/5] Problema simple...
âœ“ Problema simple (triÃ¡ngulo) creado correctamente

[3/5] CreaciÃ³n de soluciÃ³n...
âœ“ SoluciÃ³n vÃ¡lida creada y validada

[4/5] Carga DIMACS...
âŠ˜ Archivo DIMACS no encontrado (opcional)

[5/5] Evaluador...
âœ“ Evaluador funcionando: 3 colores, 0 conflictos

============================================================
  RESULTADO: 4/5 tests pasados  âœ“ EXITOSO
  Tiempo total: 0.15s
============================================================
```

### 3. Ejecutar Todos los Tests

```bash
pytest tests/ -v
```

---

## ðŸ“Š Cobertura de Tests

### Por MÃ³dulo

| MÃ³dulo | Tests | MÃ©todos | Cobertura |
|--------|-------|---------|-----------|
| `core/problem.py` | 10 | 12 | >95% |
| `core/solution.py` | 8 | 8 | >95% |
| `core/evaluation.py` | 4 | 6 | >90% |
| `operators/constructive.py` | 3 | 3 | >90% |
| `operators/improvement.py` | 5 | 3 | >90% |
| `operators/perturbation.py` | 5 | 2 | >90% |
| `metaheuristic/ils_core.py` | 6 | 5 | >85% |
| **TOTAL** | **42+** | **39** | **>90%** |

### Por Tipo

- **Unit Tests**: 19
- **Integration Tests**: 12
- **Validation Tests**: 8
- **Performance Tests**: 3+

---

## ðŸ” GuÃ­a Detallada de EjecuciÃ³n

### Ejecutar Todos los Tests

```bash
pytest tests/ -v
```

**Opciones Ãºtiles**:
```bash
# Con reporte de cobertura
pytest tests/ --cov=core --cov=operators --cov=metaheuristic --cov-report=html

# Con traceback largo
pytest tests/ -v --tb=long

# Mostrar estadÃ­sticas
pytest tests/ -v --durations=10

# Generar reporte en XML
pytest tests/ --junit-xml=report.xml

# Modo watch (ejecutar cuando cambian archivos)
pytest-watch tests/
```

### Ejecutar Tests EspecÃ­ficos

```bash
# Solo tests de una clase
pytest tests/test_core.py::TestGraphColoringProblem -v

# Solo tests que coincidan con patrÃ³n
pytest tests/ -k "feasible" -v

# Tests excepto los que coincidan con patrÃ³n
pytest tests/ -k "not dimacs" -v

# Solo los primeros 5 tests que fallen
pytest tests/ --maxfail=5 -v

# Salir en el primer fallo
pytest tests/ -x
```

### Uso del Script `run_tests.py`

```bash
# ValidaciÃ³n rÃ¡pida
python run_tests.py --quick

# Solo tests de Core
python run_tests.py --core

# Solo tests de Operadores
python run_tests.py --operators

# Solo tests de ILS
python run_tests.py --ils

# Con reporte de cobertura
python run_tests.py --coverage

# Verbose completo
python run_tests.py --verbose

# Combinaciones
python run_tests.py --core --verbose   # Tests de Core con traceback largo
python run_tests.py --coverage --verbose  # Con cobertura y traceback
```

---

## ðŸ“ Ejemplos de Tests

### Test Simple: Carga de Problema

```python
def test_vertices_count(self, triangle_problem):
    """Validar que el problema registra el nÃºmero correcto de vÃ©rtices"""
    assert triangle_problem.n_vertices == 3
```

### Test con Fixture

```python
@pytest.fixture
def triangle_problem(self):
    """Fixture: Problema simple (triÃ¡ngulo)"""
    edges = [(1, 2), (2, 3), (1, 3)]
    return GraphColoringProblem(vertices=3, edges=edges, colors_known=3)

def test_vertices_count(self, triangle_problem):
    assert triangle_problem.n_vertices == 3
```

### Test Parametrizado

```python
@pytest.mark.parametrize("vertices,edges,expected_colors", [
    (3, [(1,2), (2,3), (1,3)], 3),  # TriÃ¡ngulo
    (4, [(1,2), (2,3), (3,4), (4,1)], 2),  # Ciclo par
])
def test_various_graphs(self, vertices, edges, expected_colors):
    problem = GraphColoringProblem(vertices=vertices, edges=edges)
    # ...
```

### Test con Skip

```python
@pytest.mark.skip(reason="Requiere archivo DIMACS")
def test_load_from_dimacs(self):
    problem = GraphColoringProblem.load_from_dimacs("datasets/myciel3.col")
    assert problem.n_vertices == 11
```

---

## âœ… Checklist Pre-Testing

Antes de ejecutar los tests, asegurar que:

- [ ] `core/problem.py` estÃ¡ implementado
- [ ] `core/solution.py` estÃ¡ implementado
- [ ] `core/evaluation.py` estÃ¡ implementado
- [ ] `operators/constructive.py` estÃ¡ implementado
- [ ] `operators/improvement.py` estÃ¡ implementado
- [ ] `operators/perturbation.py` estÃ¡ implementado
- [ ] `metaheuristic/ils_core.py` estÃ¡ implementado
- [ ] Dependencias instaladas: `pip install pytest pytest-cov numpy`
- [ ] Permisos de lectura en archivos DIMACS (si aplica)
- [ ] Python 3.7+

---

## ðŸ› SoluciÃ³n de Problemas

### Error: `ModuleNotFoundError: No module named 'core'`

**SoluciÃ³n**: Ejecutar desde el directorio raÃ­z del proyecto:
```bash
cd projects/GAA-GCP-ILS-4
pytest tests/ -v
```

### Error: `ImportError` en los tests

**SoluciÃ³n**: Verificar que los mÃ³dulos estÃ¡n implementados:
```bash
ls -la core/
ls -la operators/
ls -la metaheuristic/
```

### Tests lentos

**SoluciÃ³n**: Ejecutar solo tests rÃ¡pidos:
```bash
pytest tests/ -k "not dimacs" -v
```

### Falta `pytest`

**SoluciÃ³n**: Instalar dependencias:
```bash
pip install pytest pytest-cov
```

---

## ðŸ“š Recursos Adicionales

### DocumentaciÃ³n de Pytest
- [GuÃ­a oficial](https://docs.pytest.org/)
- [Fixtures](https://docs.pytest.org/en/stable/fixture.html)
- [ParametrizaciÃ³n](https://docs.pytest.org/en/stable/how-to-parametrize.html)
- [Markers](https://docs.pytest.org/en/stable/how-to-mark.html)

### En este proyecto
- `problema_metaheuristica.md` - EspecificaciÃ³n completa
- `TESTING_SUMMARY.md` - Resumen de testing
- `scripts/test_quick.py` - ValidaciÃ³n rÃ¡pida
- `scripts/run_tests.py` - Script de ejecuciÃ³n

---

## ðŸŽ¯ Objetivos de Testing

1. **Correctitud**: Validar que componentes funcionan segÃºn especificaciÃ³n
2. **Robustez**: Detectar casos edge y manejar errores
3. **Reproducibilidad**: Permitir experimentos reproducibles con seeds
4. **DocumentaciÃ³n**: Tests sirven como ejemplos de uso
5. **RegresiÃ³n**: Prevenir cambios no deseados
6. **Confianza**: Permitir refactoring seguro

---

## ðŸ“ž Contacto y Reportes

Para reportar problemas con los tests:

1. Ejecutar con verbose: `pytest tests/ -v --tb=long`
2. Copiar el traceback completo
3. Verificar que las dependencias estÃ¡n instaladas
4. Verificar que los mÃ³dulos estÃ¡n implementados

---

**Ãšltima actualizaciÃ³n**: 31 Diciembre 2025  
**Estado**: DocumentaciÃ³n completa, tests listos para implementaciÃ³n  
**Cobertura objetivo**: >90%
